[
  {
    "objectID": "todo.html",
    "href": "todo.html",
    "title": "AIsaac",
    "section": "",
    "text": "Visualizations\n\nPlot Losses/metrics\n\nModels\n\nHuggingFace\nPytorch\n\nData Aug\n\nTorchvision\nAlbumentations\nRandom Erase"
  },
  {
    "objectID": "models.html#setup",
    "href": "models.html#setup",
    "title": "models",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "models.html#load-data",
    "href": "models.html#load-data",
    "title": "models",
    "section": "Load Data",
    "text": "Load Data\n\nxmean,xstd = 0.28, 0.35\n\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)\n_dataset = sample_dataset_dict(_dataset)\ndls = DataLoaders.from_dataset_dict(_dataset, 64, num_workers=4)\nxb = fc.first(dls.train)[0]"
  },
  {
    "objectID": "models.html#models",
    "href": "models.html#models",
    "title": "models",
    "section": "Models",
    "text": "Models\n\nsource\n\nget_model_timm\n\n get_model_timm (model_name, pretrained=False, pretrained_cfg=None,\n                 checkpoint_path='', scriptable=None, exportable=None,\n                 no_jit=None)\n\nLoads model from timm, see timm.list_models for options\n\nmodel = get_model_timm('resnet18', pretrained=True,num_classes=10,in_chans=1).to(def_device)\nfc.test_eq(model(fc.first(dls.train)[0].to(def_device)).shape,(64,10))\n\n\nsource\n\n\nget_model_conv\n\n get_model_conv (act=<class 'torch.nn.modules.activation.ReLU'>, nfs=None,\n                 norm=None)\n\n\nsource\n\n\nconv\n\n conv (ni, nf, kernel_size=3, stride=2, act=<class\n       'torch.nn.modules.activation.ReLU'>, norm=None, bias=None)\n\n\nmodel = get_model_conv()\nfc.test_eq(model(fc.first(dls.train)[0].to(def_device)).shape,(64,10))"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\n\n\n get_images (path, suffixes=('.jpg', '.jpeg', '.png'))\n\n\nsource\n\n\n\n\n set_seed (seed, deterministic=False)\n\n\nsource\n\n\n\n\n inplace (f)\n\nReturn the object passed to the function for in place mods\n\nsource\n\n\n\n\n mask2idxs (mask)\n\n\nmask = fc.L(True, False, True, False, True)\nfc.test_eq(mask2idxs(mask), [0, 2, 4])\nfc.test_eq(mask2idxs(~mask), [1,3])\n\n\nsource\n\n\n\n\nsource\n\n\n\n\n retrieve_global_name (var)"
  },
  {
    "objectID": "utils.html#memory",
    "href": "utils.html#memory",
    "title": "utils",
    "section": "Memory",
    "text": "Memory\n\nMuch of this code is taken from the minai library from fastai\n\n\nsource\n\nclean_ipython_hist\n\n clean_ipython_hist ()\n\n\nsource\n\n\nclean_traceback\n\n clean_traceback ()\n\nObjects in tracebacks are stored in memory, even cuda memory. This clears that traceback memory up\n\nsource\n\n\nclean_memory\n\n clean_memory ()\n\nCleans all memory from hist and tracebacks"
  },
  {
    "objectID": "utils.html#device-management",
    "href": "utils.html#device-management",
    "title": "utils",
    "section": "Device Management",
    "text": "Device Management\n\nsource\n\nto_cpu\n\n to_cpu (x)\n\n\nsource\n\n\nto_device\n\n to_device (x:<built-inmethodtensoroftypeobjectat0x7faab94b3460>,\n            device='cpu')"
  },
  {
    "objectID": "utils.html#matplotlib-helpers",
    "href": "utils.html#matplotlib-helpers",
    "title": "utils",
    "section": "MatplotLib Helpers",
    "text": "MatplotLib Helpers\n\nMuch of this code is taken from the minai library from fastai\n\n/opt/hostedtoolcache/Python/3.10.10/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Other Parameters\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.10/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section See Also\n  else: warn(msg)\n/opt/hostedtoolcache/Python/3.10.10/x64/lib/python3.10/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Notes\n  else: warn(msg)\n\nsource\n\nshow_image\n\n show_image (im, ax=None, figsize=None, title=None, noframe=True,\n             cmap=None, norm=None, aspect=None, interpolation=None,\n             alpha=None, vmin=None, vmax=None, origin=None, extent=None,\n             interpolation_stage=None, filternorm=True, filterrad=4.0,\n             resample=None, url=None, data=None)\n\nShow a PIL or PyTorch image on ax + Moves to cpu & detach + converts to numpy + remove axis ticks\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nim\n\n\n\n\n\nax\nNoneType\nNone\n\n\n\nfigsize\nNoneType\nNone\n\n\n\ntitle\nNoneType\nNone\n\n\n\nnoframe\nbool\nTrue\n\n\n\ncmap\nNoneType\nNone\nThe Colormap instance or registered colormap name used to map scalar datato colors.This parameter is ignored if X is RGB(A).\n\n\nnorm\nNoneType\nNone\nThe normalization method used to scale scalar data to the [0, 1] rangebefore mapping to colors using cmap. By default, a linear scaling isused, mapping the lowest value to 0 and the highest to 1.If given, this can be one of the following:- An instance of .Normalize or one of its subclasses (see :doc:/tutorials/colors/colormapnorms).- A scale name, i.e. one of “linear”, “log”, “symlog”, “logit”, etc. For a list of available scales, call matplotlib.scale.get_scale_names(). In that case, a suitable .Normalize subclass is dynamically generated and instantiated.This parameter is ignored if X is RGB(A).\n\n\naspect\nNoneType\nNone\nThe aspect ratio of the Axes. This parameter is particularlyrelevant for images since it determines whether data pixels aresquare.This parameter is a shortcut for explicitly calling.Axes.set_aspect. See there for further details.- ‘equal’: Ensures an aspect ratio of 1. Pixels will be square (unless pixel sizes are explicitly made non-square in data coordinates using extent).- ‘auto’: The Axes is kept fixed and the aspect is adjusted so that the data fit in the Axes. In general, this will result in non-square pixels.\n\n\ninterpolation\nNoneType\nNone\nThe interpolation method used.Supported values are ‘none’, ‘antialiased’, ‘nearest’, ‘bilinear’,‘bicubic’, ‘spline16’, ‘spline36’, ‘hanning’, ‘hamming’, ‘hermite’,‘kaiser’, ‘quadric’, ‘catrom’, ‘gaussian’, ‘bessel’, ‘mitchell’,‘sinc’, ‘lanczos’, ‘blackman’.If interpolation is ‘none’, then no interpolation is performedon the Agg, ps, pdf and svg backends. Other backends will fall backto ‘nearest’. Note that most SVG renderers perform interpolation atrendering and that the default interpolation method they implementmay differ.If interpolation is the default ‘antialiased’, then ‘nearest’interpolation is used if the image is upsampled by more than afactor of three (i.e. the number of display pixels is at leastthree times the size of the data array). If the upsampling rate issmaller than 3, or the image is downsampled, then ‘hanning’interpolation is used to act as an anti-aliasing filter, unless theimage happens to be upsampled by exactly a factor of two or one.See:doc:/gallery/images_contours_and_fields/interpolation_methodsfor an overview of the supported interpolation methods, and:doc:/gallery/images_contours_and_fields/image_antialiasing fora discussion of image antialiasing.Some interpolation methods require an additional radius parameter,which can be set by filterrad. Additionally, the antigrain imageresize filter is controlled by the parameter filternorm.\n\n\nalpha\nNoneType\nNone\nThe alpha blending value, between 0 (transparent) and 1 (opaque).If alpha is an array, the alpha blending values are applied pixelby pixel, and alpha must have the same shape as X.\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\nPlace the [0, 0] index of the array in the upper left or lowerleft corner of the Axes. The convention (the default) ‘upper’ istypically used for matrices and images.Note that the vertical axis points upward for ‘lower’but downward for ‘upper’.See the :doc:/tutorials/intermediate/imshow_extent tutorial forexamples and a more detailed description.\n\n\nextent\nNoneType\nNone\nThe bounding box in data coordinates that the image will fill.These values may be unitful and match the units of the Axes.The image is stretched individually along x and y to fill the box.The default extent is determined by the following conditions.Pixels have unit size in data coordinates. Their centers are oninteger coordinates, and their center coordinates range from 0 tocolumns-1 horizontally and from 0 to rows-1 vertically.Note that the direction of the vertical axis and thus the defaultvalues for top and bottom depend on origin:- For origin == 'upper' the default is (-0.5, numcols-0.5, numrows-0.5, -0.5).- For origin == 'lower' the default is (-0.5, numcols-0.5, -0.5, numrows-0.5).See the :doc:/tutorials/intermediate/imshow_extent tutorial forexamples and a more detailed description.\n\n\ninterpolation_stage\nNoneType\nNone\nIf ‘data’, interpolationis carried out on the data provided by the user. If ‘rgba’, theinterpolation is carried out after the colormapping has beenapplied (visual interpolation).\n\n\nfilternorm\nbool\nTrue\nA parameter for the antigrain image resize filter (see theantigrain documentation). If filternorm is set, the filternormalizes integer values and corrects the rounding errors. Itdoesn’t do anything with the source floating point values, itcorrects only integers according to the rule of 1.0 which meansthat any sum of pixel weights must be equal to 1.0. So, thefilter function must produce a graph of the proper shape.\n\n\nfilterrad\nfloat\n4.0\nThe filter radius for filters that have a radius parameter, i.e.when interpolation is one of: ‘sinc’, ‘lanczos’ or ‘blackman’.\n\n\nresample\nNoneType\nNone\nWhen True, use a full resampling method. When False, onlyresample when the output image is larger than the input image.\n\n\nurl\nNoneType\nNone\nSet the url of the created .AxesImage. See .Artist.set_url.\n\n\ndata\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nsubplots\n\n subplots (nrows:int=1, ncols:int=1, figsize:tuple=None, imsize:int=3,\n           suptitle:str=None, sharex=False, sharey=False, squeeze=True,\n           width_ratios=None, height_ratios=None, subplot_kw=None,\n           gridspec_kw=None, **kwargs)\n\nA figure and set of subplots to display images of imsize inches\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnrows\nint\n1\nNumber of rows in returned axes grid\n\n\nncols\nint\n1\nNumber of columns in returned axes grid\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool\nFalse\n\n\n\nsharey\nbool\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nNoneType\nNone\n\n\n\nheight_ratios\nNoneType\nNone\n\n\n\nsubplot_kw\nNoneType\nNone\n\n\n\ngridspec_kw\nNoneType\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nget_grid\n\n get_grid (n:int, nrows:int=None, ncols:int=None, title:str=None,\n           weight:str='bold', size:int=14, figsize:tuple=None,\n           imsize:int=3, suptitle:str=None, sharex=False, sharey=False,\n           squeeze=True, width_ratios=None, height_ratios=None,\n           subplot_kw=None, gridspec_kw=None)\n\nReturn a grid of n axes, rows by cols\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n\nNumber of axes\n\n\nnrows\nint\nNone\nNumber of rows, defaulting to int(math.sqrt(n))\n\n\nncols\nint\nNone\nNumber of columns, defaulting to ceil(n/rows)\n\n\ntitle\nstr\nNone\nIf passed, title set to the figure\n\n\nweight\nstr\nbold\nTitle font weight\n\n\nsize\nint\n14\nTitle font size\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool\nFalse\n\n\n\nsharey\nbool\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nNoneType\nNone\n\n\n\nheight_ratios\nNoneType\nNone\n\n\n\nsubplot_kw\nNoneType\nNone\n\n\n\ngridspec_kw\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nshow_images\n\n show_images (ims:list, nrows:int|None=None, ncols:int|None=None,\n              titles:list|None=None, figsize:tuple=None, imsize:int=3,\n              suptitle:str=None, sharex=False, sharey=False, squeeze=True,\n              width_ratios=None, height_ratios=None, subplot_kw=None,\n              gridspec_kw=None)\n\nShow all images ims as subplots with rows using titles\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nims\nlist\n\nImages to show\n\n\nnrows\nint | None\nNone\nNumber of rows in grid\n\n\nncols\nint | None\nNone\nNumber of columns in grid (auto-calculated if None)\n\n\ntitles\nlist | None\nNone\nOptional list of titles for each image\n\n\nfigsize\ntuple\nNone\nWidth, height in inches of the returned figure\n\n\nimsize\nint\n3\nSize (in inches) of images that will be displayed in the returned figure\n\n\nsuptitle\nstr\nNone\nTitle to be set to returned figure\n\n\nsharex\nbool\nFalse\n\n\n\nsharey\nbool\nFalse\n\n\n\nsqueeze\nbool\nTrue\n\n\n\nwidth_ratios\nNoneType\nNone\n\n\n\nheight_ratios\nNoneType\nNone\n\n\n\nsubplot_kw\nNoneType\nNone\n\n\n\ngridspec_kw\nNoneType\nNone"
  },
  {
    "objectID": "dataloaders.html",
    "href": "dataloaders.html",
    "title": "dataloaders",
    "section": "",
    "text": "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\nmpl.rcParams['image.cmap'] = 'gray'\nlogging.disable(logging.WARNING)\n\nset_seed(42)\n\n\nsource\n\nget_dataloaders\n\n get_dataloaders (train_dataset, valid_dataset, batch_size,\n                  shuffle:Optional[bool]=None, sampler:Union[torch.utils.d\n                  ata.sampler.Sampler,Iterable,NoneType]=None, batch_sampl\n                  er:Union[torch.utils.data.sampler.Sampler[Sequence],Iter\n                  able[Sequence],NoneType]=None, num_workers:int=0,\n                  collate_fn:Optional[Callable[[List[~T]],Any]]=None,\n                  pin_memory:bool=False, drop_last:bool=False,\n                  timeout:float=0,\n                  worker_init_fn:Optional[Callable[[int],NoneType]]=None,\n                  multiprocessing_context=None, generator=None,\n                  prefetch_factor:int=2, persistent_workers:bool=False,\n                  pin_memory_device:str='')\n\n\nsource\n\n\ncollate_dataset_dict\n\n collate_dataset_dict (dataset)\n\n\nsource\n\n\nDataLoaders\n\n DataLoaders (train, valid, n_inp=1)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nsample_dataset_dict\n\n sample_dataset_dict (dataset, sample_sizes=(500, 500))\n\n\nxmean,xstd = 0.28, 0.35\n\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)\n_dataset = sample_dataset_dict(_dataset)\ndls = DataLoaders.from_dataset_dict(_dataset, 64, num_workers=4)\n\n100%|██████████| 2/2 [00:00<00:00, 364.98it/s]\n\n\n\n_dataset['train']\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 500\n})\n\n\n\ndls.train.dataset\n\nDataset({\n    features: ['image', 'label'],\n    num_rows: 500\n})\n\n\n\ndls.show_batch()"
  },
  {
    "objectID": "recording.html",
    "href": "recording.html",
    "title": "Recording",
    "section": "",
    "text": "xmean,xstd = 0.28, 0.35\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = sample_dataset_dict(load_dataset('fashion_mnist').with_transform(transformi))\n\n100%|██████████| 2/2 [00:00<00:00, 354.55it/s]"
  },
  {
    "objectID": "recording.html#core",
    "href": "recording.html#core",
    "title": "Recording",
    "section": "Core",
    "text": "Core\n\nsource\n\nMetricsCB\n\n MetricsCB (**metrics)\n\nCallback to track train/valid loss + metrics\n\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.Adam, \n                  get_model_conv(), \n                  callbacks=[BasicTrainCB(),MetricsCB(accuracy=MulticlassAccuracy()), DeviceCB()])\n\n\ntrainer.fit()\n\n\nfc.test_eq(pd.DataFrame(trainer.MetricsCB.losses_epoch).shape,(3,2))\nfc.test_eq(pd.DataFrame(trainer.MetricsCB.metrics_epoch).shape,(3,1))\nfc.test_eq(pd.DataFrame(trainer.MetricsCB.losses_batch).shape,(9,2))\n\n\nsource\n\n\nProgressCB\n\n ProgressCB ()\n\nCallback to display progress while training\n\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.Adam, \n                  get_model_conv(), \n                  callbacks=[BasicTrainCB(),MetricsCB(accuracy=MulticlassAccuracy()), DeviceCB(),OneBatchCB(),ProgressCB()])\n\n\ntrainer.fit()\n\n\n\n\n\n\n    \n      \n      0.00% [0/3 00:00<?]\n    \n    \n\n\n    \n      \n      0.00% [0/235 00:00<? Training]"
  },
  {
    "objectID": "cb_groups.html",
    "href": "cb_groups.html",
    "title": "Recording",
    "section": "",
    "text": "xmean,xstd = 0.28, 0.35\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)\n\n\n\n\n\ndls = DataLoaders.from_dataset_dict(_dataset, 256, num_workers=4)\n\n\nsource\n\nCoreCBs\n\n CoreCBs (device='cpu', module_filter=<function noop>, **metrics)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.Adam, \n                  get_model_conv(),\n                  callbacks=[CoreCBs(Accuracy=MulticlassAccuracy()),OneBatchCB()])\n\n\ntrainer.fit()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      0\n      2.3023\n      2.293682\n      0.203776\n    \n  \n\n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      1\n      2.288657\n      2.277659\n      0.448568\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      2\n      2.265343\n      2.246487\n      0.443359"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "training",
    "section": "",
    "text": "xmean,xstd = 0.28, 0.35\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)\n\n100%|██████████| 2/2 [00:00<00:00, 345.18it/s]"
  },
  {
    "objectID": "training.html#core",
    "href": "training.html#core",
    "title": "training",
    "section": "Core",
    "text": "Core\n\nsource\n\nOneBatchCB\n\n OneBatchCB ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nBasicTrainCB\n\n BasicTrainCB ()\n\nCallback for basic pytorch training loop\n\nsource\n\n\nDeviceCB\n\n DeviceCB (device='cpu')\n\nCallback to train on specific device\n\nsource\n\n\nMomentumTrainCB\n\n MomentumTrainCB (momentum)\n\nCallback for basic pytorch training loop\n\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.Adam, \n                  get_model_conv(), \n                  callbacks=[BasicTrainCB(), DeviceCB(),OneBatchCB()])\n\n\ntrainer.fit()\n\n\ntrainer.summarize_callbacks()\n\n\n\n\n\n  \n    \n      \n      Step\n      Callback\n      Doc String\n    \n  \n  \n    \n      \n      before_fit\n      DeviceCB\n      Moves model to device\n    \n    \n      \n      before_batch\n      DeviceCB\n      moves batch to device\n    \n    \n      \n      predict\n      BasicTrainCB\n      \n    \n    \n      \n      get_loss\n      BasicTrainCB\n      \n    \n    \n      \n      backward\n      BasicTrainCB\n      \n    \n    \n      \n      step\n      BasicTrainCB\n      \n    \n    \n      \n      zero_grad\n      BasicTrainCB\n      \n    \n    \n      \n      after_batch\n      OneBatchCB"
  },
  {
    "objectID": "training.html#optimization",
    "href": "training.html#optimization",
    "title": "training",
    "section": "Optimization",
    "text": "Optimization\n\nsource\n\nBaseSchedulerCB\n\n BaseSchedulerCB (scheduler_func)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nEpochSchedulerCB\n\n EpochSchedulerCB (scheduler_func)\n\nSteps scheduler\n\nsource\n\n\nBatchSchedulerCB\n\n BatchSchedulerCB (scheduler_func)\n\nSteps scheduler\n\nsource\n\n\nOneCycleSchedulerCB\n\n OneCycleSchedulerCB (pct_start=0.3, anneal_strategy='cos',\n                      cycle_momentum=True, base_momentum=0.85,\n                      max_momentum=0.95, div_factor=25.0,\n                      final_div_factor=10000.0, three_phase=False,\n                      last_epoch=-1, verbose=False)\n\nSteps scheduler"
  },
  {
    "objectID": "training.html#acceleration",
    "href": "training.html#acceleration",
    "title": "training",
    "section": "Acceleration",
    "text": "Acceleration\n\nsource\n\nAccelerateCB\n\n AccelerateCB (mixed_precision='fp16')\n\nCallback for basic pytorch training loop"
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "visalizations",
    "section": "",
    "text": "xmean,xstd = 0.28, 0.35\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = sample_dataset_dict(load_dataset('fashion_mnist').with_transform(transformi),(2000,500))\ndls = DataLoaders.from_dataset_dict(_dataset, 256, num_workers=4)\n\n100%|██████████| 2/2 [00:00<00:00, 356.19it/s]"
  },
  {
    "objectID": "visualization.html#learning-rate-finder",
    "href": "visualization.html#learning-rate-finder",
    "title": "visalizations",
    "section": "Learning Rate Finder",
    "text": "Learning Rate Finder\n\nsource\n\nLRFinderCB\n\n LRFinderCB (lr_mult=1.3)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.SGD, \n                  get_model_conv(), \n                  callbacks=[BasicTrainCB(),DeviceCB(),MetricsCB(Accuracy=MulticlassAccuracy())])\n\n\ntrainer.fit(callbacks=[LRFinderCB()])"
  },
  {
    "objectID": "trainer.html",
    "href": "trainer.html",
    "title": "trainer",
    "section": "",
    "text": "xmean,xstd = 0.28, 0.35\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)\n\n100%|██████████| 2/2 [00:00<00:00, 344.12it/s]"
  },
  {
    "objectID": "trainer.html#base-trainer",
    "href": "trainer.html#base-trainer",
    "title": "trainer",
    "section": "Base Trainer",
    "text": "Base Trainer\n\nsource\n\nCancelEpochException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelBatchException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCancelFitException\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nCallback\n\n Callback ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nsummarize_callbacks\n\n summarize_callbacks (trainer)\n\n\ndef summarize_model(model,batch,row_settings=(\"var_names\",),verbose=0,depth=3,col_names=(\"input_size\",\"output_size\",\"kernel_size\")):\n    \n    # Other useful columns: \"num_params\",\"mult_adds\"\n    return torchinfo.summary(model,input_data=batch,row_settings=row_settings,verbose=verbose,depth=depth,col_names=col_names)\n\n\nsource\n\n\nTrainer\n\n Trainer (dls, loss_func, opt_func, model, callbacks)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.Adam, \n                  get_model_conv(),\n                  callbacks=[])\n\n\ntrainer.summarize_model()\n\n===================================================================================================================\nLayer (type (var_name))                  Input Shape               Output Shape              Kernel Shape\n===================================================================================================================\nSequential (Sequential)                  [500, 1, 28, 28]          [500, 10]                 --\n├─Sequential (0)                         [500, 1, 28, 28]          [500, 8, 14, 14]          --\n│    └─Conv2d (0)                        [500, 1, 28, 28]          [500, 8, 14, 14]          [3, 3]\n│    └─ReLU (1)                          [500, 8, 14, 14]          [500, 8, 14, 14]          --\n├─Sequential (1)                         [500, 8, 14, 14]          [500, 16, 7, 7]           --\n│    └─Conv2d (0)                        [500, 8, 14, 14]          [500, 16, 7, 7]           [3, 3]\n│    └─ReLU (1)                          [500, 16, 7, 7]           [500, 16, 7, 7]           --\n├─Sequential (2)                         [500, 16, 7, 7]           [500, 32, 4, 4]           --\n│    └─Conv2d (0)                        [500, 16, 7, 7]           [500, 32, 4, 4]           [3, 3]\n│    └─ReLU (1)                          [500, 32, 4, 4]           [500, 32, 4, 4]           --\n├─Sequential (3)                         [500, 32, 4, 4]           [500, 64, 2, 2]           --\n│    └─Conv2d (0)                        [500, 32, 4, 4]           [500, 64, 2, 2]           [3, 3]\n│    └─ReLU (1)                          [500, 64, 2, 2]           [500, 64, 2, 2]           --\n├─Sequential (4)                         [500, 64, 2, 2]           [500, 10, 1, 1]           --\n│    └─Conv2d (0)                        [500, 64, 2, 2]           [500, 10, 1, 1]           [3, 3]\n├─Flatten (5)                            [500, 10, 1, 1]           [500, 10]                 --\n===================================================================================================================\nTotal params: 30,154\nTrainable params: 30,154\nNon-trainable params: 0\nTotal mult-adds (M): 113.45\n===================================================================================================================\nInput size (MB): 1.57\nForward/backward pass size (MB): 12.52\nParams size (MB): 0.12\nEstimated Total Size (MB): 14.21\n==================================================================================================================="
  },
  {
    "objectID": "trainer.html#trainer-summaries",
    "href": "trainer.html#trainer-summaries",
    "title": "trainer",
    "section": "Trainer Summaries",
    "text": "Trainer Summaries"
  },
  {
    "objectID": "augmentation.html",
    "href": "augmentation.html",
    "title": "Recording",
    "section": "",
    "text": "xmean,xstd = 0.28, 0.35\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)\n\n100%|██████████| 2/2 [00:00<00:00, 507.20it/s]\n\n\n\ndls = DataLoaders.from_dataset_dict(_dataset, 256, num_workers=4)\n\n\nsource\n\nBatchAugmentationCB\n\n BatchAugmentationCB (tfms)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nUnNormalize\n\n UnNormalize (mean, std)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nshow_doc\n\n show_doc (sym, renderer=None, name:str|None=None, title_level:int=3)\n\nShow signature and docstring for sym\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsym\n\n\nSymbol to document\n\n\nrenderer\nNoneType\nNone\nOptional renderer (defaults to markdown)\n\n\nname\nstr | None\nNone\nOptionally override displayed name of sym\n\n\ntitle_level\nint\n3\nHeading level to use for symbol name\n\n\n\n\ntfms = [torch.nn.Sequential(transforms.RandomVerticalFlip(1),transforms.RandomErasing(1))]\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.Adam, \n                  get_model_conv(),\n                  callbacks=[BasicTrainCB(),DeviceCB(),BatchAugmentationCB(tfms)])\n\n\ntrainer.show_image_batch(3)"
  },
  {
    "objectID": "Tutorials/dreambooth.html",
    "href": "Tutorials/dreambooth.html",
    "title": "Dreambooth",
    "section": "",
    "text": "from AIsaac.all import *\nimport fastcore.all as fc\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, PretrainedConfig,CLIPTextModel\nfrom accelerate import Accelerator\nimport xformers # May need dev version\nfrom diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel, StableDiffusionPipeline\nfrom diffusers.optimization import get_scheduler\nfrom functools import partial\n\nimport torchvision\nfrom torchvision import transforms\nimport torchvision.transforms.functional as TF\n\nfrom PIL import Image\nimport itertools, math, hashlib, random\nfrom pathlib import Path\nfrom itertools import zip_longest\n\nfrom IPython.display import clear_output\n\n/home/python3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nA matching Triton is not available, some optimizations will not be enabled.\nError caught was: No module named 'triton'"
  },
  {
    "objectID": "Tutorials/dreambooth.html#utils",
    "href": "Tutorials/dreambooth.html#utils",
    "title": "Dreambooth",
    "section": "Utils",
    "text": "Utils\n\ndef decode(tokenizer, tokens, start_after='<|startoftext|>', end_at='<|endoftext|>'):\n    _decoded = ds.tokenizer.decode(tokens)\n    return _decoded[len(start_after):_decoded.find(end_at)].strip()\n\ndenorm = UnNormalize([.5],[.5])\n\n\npretrained_model=\"CompVis/stable-diffusion-v1-4\" \ncls_img_dir = Path(\"dreambooth/class\")\nimg_dir = Path(\"dreambooth/cropped\")"
  },
  {
    "objectID": "Tutorials/dreambooth.html#generate-class-images",
    "href": "Tutorials/dreambooth.html#generate-class-images",
    "title": "Dreambooth",
    "section": "Generate Class Images",
    "text": "Generate Class Images\n\ndef hash_img(img): return hashlib.sha1(img.tobytes()).hexdigest()\n\n\npipeline = DiffusionPipeline.from_pretrained(pretrained_model,\n                        torch_dtype=torch.float32,safety_checker=None,revision=None).to(\"cuda\")\n\nfor i in range(10):\n    images = pipeline([\"a photo of dog\"]*8).images\n    path = cls_img_dir / ('train' if random.random()<0.8 else 'valid')\n    for i, image in enumerate(images): image.save(path/f\"{hash_img(image)}.jpg\")\n    \ndel pipeline\nclean_memory()"
  },
  {
    "objectID": "Tutorials/dreambooth.html#fine-tuning",
    "href": "Tutorials/dreambooth.html#fine-tuning",
    "title": "Dreambooth",
    "section": "Fine Tuning",
    "text": "Fine Tuning\n\nData Loading\n\ndef transform(image,xmean=0.5,xstd=0.5): return (TF.to_tensor(TF.resize(image,512))-xmean)/xstd\n\nclass DBDataset(Dataset):\n    def __init__(self, class_imgs, class_prompt, main_imgs, main_prompt): \n        fc.store_attr('class_imgs,class_prompt,main_imgs,main_prompt')\n    def __len__(self): return len(self.main_imgs)\n    def __getitem__(self, idx):\n        cls_img, main_img = map(Image.open,[self.class_imgs[idx],self.main_imgs[idx]])\n        cls_img, main_img = map(transform,[cls_img, main_img])               \n        return (torch.stack([main_img,cls_img]), \n                (self.main_prompt,self.class_prompt))\n\n\ntrn_ds = DBDataset(get_images(cls_img_dir/'train'),\"a photo of dog\",get_images(img_dir/'train'),\"a photo of sks dog\")\nval_ds = DBDataset(get_images(cls_img_dir/'valid')[:10],\"a photo of dog\",get_images(img_dir/'valid'),\"a photo of sks dog\")\n\n\nfrom torch.utils.data import DataLoader\ndls = DataLoaders(*map(partial(DataLoader,batch_size=None),[trn_ds,val_ds]))\n\n\n\nData Augmentation\n\nimg_tfms = transforms.Compose([transforms.RandomCrop(512),\n                               transforms.Normalize([0.5], [0.5])])\n\ntokenizer = AutoTokenizer.from_pretrained(pretrained_model,\n                                          subfolder=\"tokenizer\",\n                                          revision=None,use_fast=False,)\ndef lbl_tfms(yb,tokenizer=tokenizer):\n    out = tokenizer(yb,\n                    truncation=True, padding=\"max_length\",\n                    max_length=tokenizer.model_max_length,\n                    return_tensors=\"pt\",)\n    return out.input_ids\n\n\n\nModel\n\nclass DreamBoothModel(nn.Module):\n    def __init__(self): \n        super().__init__()\n        self.unet = UNet2DConditionModel.from_pretrained(pretrained_model, \n                                                         subfolder=\"unet\", \n                                                         revision=None)\n        self.text_encoder = CLIPTextModel.from_pretrained(pretrained_model, \n                                                          subfolder=\"text_encoder\", \n                                                          revision=None)\n        \n    def forward(self,x):        \n        encoder_hidden_states = self.text_encoder(x[1])[0]\n        return self.unet(x[5], x[4], encoder_hidden_states).sample.float()\n\n\n\nTraining CB\n\nclass DreamBoothTrainCB(AccelerateCB):\n    def __init__(self, mixed_precision=\"fp16\",class_image_weight=0.5):\n        self.class_image_weight = class_image_weight\n        self.acc = Accelerator(mixed_precision=mixed_precision)\n        \n    def before_fit(self, trainer):\n        '''Wraps model, opt, data in accelerate'''\n        trainer.model,trainer.opt,trainer.dls.train,trainer.dls.valid = self.acc.prepare(\n                 trainer.model, trainer.opt, trainer.dls.train, trainer.dls.valid)\n        \n        trainer.noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n        trainer.vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\", revision=None)        \n        trainer.vae.requires_grad_(False)\n        trainer.vae.to(self.acc.device, dtype=torch.float16)\n\n    def before_batch(self,trainer):\n        '''Calculates latents, noise, timesteps, noisy_latents'''        \n        vae, scaling_factor = trainer.vae, trainer.vae.config.scaling_factor\n        trainer.batch.append(vae.encode(trainer.batch[0].to(dtype=torch.float16)).latent_dist.sample()*scaling_factor )\n        trainer.batch[0] = to_cpu(trainer.batch[0]) \n        trainer.batch.append(torch.randn_like(trainer.batch[2]).float())\n        num_ts = trainer.noise_scheduler.config.num_train_timesteps\n        trainer.batch.append(torch.randint(0, num_ts, (trainer.batch[2].shape[0],), device=trainer.batch[2].device))\n        trainer.batch.append(trainer.noise_scheduler.add_noise(trainer.batch[2], trainer.batch[3], trainer.batch[4]))\n                \n    def predict(self,trainer): trainer.preds = trainer.model(trainer.batch)\n\n    def get_loss(self,trainer): \n        '''Calculates loss for instance and class images'''\n        model_pred, model_pred_prior = torch.chunk(trainer.preds, 2, dim=0)\n        target, target_prior = torch.chunk(trainer.batch[3], 2, dim=0)\n        loss = trainer.loss_func(model_pred,target, reduction=\"mean\")\n        prior_loss = trainer.loss_func(model_pred_prior, target_prior, reduction=\"mean\")\n        trainer.loss = ((1 - self.class_image_weight) * loss) +  (self.class_image_weight * prior_loss)\n                \n    def backward(self,trainer):\n        '''backward pass + clip_grad'''\n        self.acc.backward(trainer.loss)\n        if self.acc.sync_gradients: self.acc.clip_grad_norm_(trainer.model.parameters(), 1.)\n   \n    def zero_grad(self,trainer): trainer.opt.zero_grad(set_to_none=True)\n\n\n\nFine Tune\n\ntrainer = Trainer(dls,fc.bind(F.mse_loss,reduction=\"mean\"),torch.optim.AdamW,\n                    DreamBoothModel(),\n                    callbacks=[DreamBoothTrainCB(class_image_weight=.25),\n                               OneCycleSchedulerCB(),\n                               BatchAugmentationCB((img_tfms,lbl_tfms)),\n                               DeviceCB(),\n                               MetricsCB(),\n                               ProgressCB()]\n                 )\n\n\nimport pandas as pd\npd.set_option('display.max_colwidth', None)\ntrainer.summarize_callbacks()\n\n\n\n\n\n  \n    \n      \n      Step\n      Callback\n      Doc String\n    \n  \n  \n    \n      \n      before_fit\n      OneCycleSchedulerCB\n      Initializes Scheduler\n    \n    \n      \n      before_fit\n      DeviceCB\n      Moves model to device\n    \n    \n      \n      before_fit\n      ProgressCB\n      Initialize Fit Progress Bar\n    \n    \n      \n      before_fit\n      DreamBoothTrainCB\n      Wraps model, opt, data in accelerate\n    \n    \n      \n      before_epoch\n      ProgressCB\n      Initialize Epoch Progress Bar\n    \n    \n      \n      before_batch\n      BatchAugmentationCB\n      applies tfms in tfms list to appropriate items in batch\n    \n    \n      \n      before_batch\n      DeviceCB\n      moves batch to device\n    \n    \n      \n      before_batch\n      DreamBoothTrainCB\n      Calculates latents, noise, timesteps, noisy_latents\n    \n    \n      \n      predict\n      DreamBoothTrainCB\n      \n    \n    \n      \n      get_loss\n      DreamBoothTrainCB\n      Calculates loss for instance and class images\n    \n    \n      \n      backward\n      DreamBoothTrainCB\n      backward pass + clip_grad\n    \n    \n      \n      step\n      DreamBoothTrainCB\n      \n    \n    \n      \n      zero_grad\n      DreamBoothTrainCB\n      \n    \n    \n      \n      after_batch\n      OneCycleSchedulerCB\n      \n    \n    \n      \n      after_batch\n      MetricsCB\n      stores losses and metrics for batch\n    \n    \n      \n      cleanup_epoch\n      MetricsCB\n      compute metrics and append to epoch stats and display\n    \n    \n      \n      cleanup_epoch\n      ProgressCB\n      Display Loss and Metric\n    \n  \n\n\n\n\n\ntrainer.fit(50,lr=5e-6)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      train\n      valid\n    \n  \n  \n    \n      0\n      0.125696\n      0.230318\n    \n  \n\n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      1\n      0.232612\n      0.123332\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      2\n      0.066541\n      0.182517\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      3\n      0.161011\n      0.098295\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      4\n      0.112926\n      0.091012\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      5\n      0.185095\n      0.196490\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      6\n      0.101598\n      0.122647\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      7\n      0.106308\n      0.091670\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      8\n      0.169324\n      0.082453\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      9\n      0.107214\n      0.180169\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      10\n      0.118429\n      0.126893\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      11\n      0.181714\n      0.133622\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      12\n      0.159709\n      0.097678\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      13\n      0.235059\n      0.057040\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      14\n      0.072090\n      0.101951\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      15\n      0.153997\n      0.150678\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      16\n      0.110091\n      0.086789\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      17\n      0.141006\n      0.186996\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      18\n      0.218605\n      0.127136\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      19\n      0.084883\n      0.158228\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      20\n      0.117560\n      0.103090\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      21\n      0.117552\n      0.185017\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      22\n      0.148583\n      0.104637\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      23\n      0.173988\n      0.156446\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      24\n      0.176349\n      0.219658\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      25\n      0.091933\n      0.122982\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      26\n      0.114160\n      0.142323\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      27\n      0.127860\n      0.082527\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      28\n      0.173347\n      0.105327\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      29\n      0.185406\n      0.194762\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      30\n      0.105407\n      0.102940\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      31\n      0.130632\n      0.227918\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      32\n      0.140506\n      0.168911\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      33\n      0.046199\n      0.108546\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      34\n      0.107185\n      0.076605\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      35\n      0.133266\n      0.136607\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      36\n      0.120615\n      0.181331\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      37\n      0.170491\n      0.092342\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      38\n      0.198938\n      0.142778\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      39\n      0.073053\n      0.131693\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      40\n      0.087147\n      0.098189\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      41\n      0.178266\n      0.205903\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      42\n      0.099110\n      0.110045\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      43\n      0.133939\n      0.302774\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      44\n      0.085011\n      0.180767\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      45\n      0.138991\n      0.199966\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      46\n      0.112717\n      0.085119\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      47\n      0.276444\n      0.096584\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      48\n      0.132728\n      0.187248\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n    \n  \n  \n    \n      49\n      0.149210\n      0.139900"
  },
  {
    "objectID": "Tutorials/dreambooth.html#save-model",
    "href": "Tutorials/dreambooth.html#save-model",
    "title": "Dreambooth",
    "section": "Save Model",
    "text": "Save Model\n\noutput_dir=\"dreambooth/out\"\nuw_model = trainer.DreamBoothTrainCB.acc.unwrap_model(trainer.model)\npipeline = DiffusionPipeline.from_pretrained(\n    pretrained_model,\n    unet=uw_model.unet,\n    text_encoder=uw_model.text_encoder,\n    revision=None,\n)\npipeline.save_pretrained(output_dir)\n\nFetching 16 files: 100%|██████████| 16/16 [00:00<00:00, 27962.03it/s]\n/home/python3.10/lib/python3.10/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n  warnings.warn("
  },
  {
    "objectID": "Tutorials/dreambooth.html#generate-new-images",
    "href": "Tutorials/dreambooth.html#generate-new-images",
    "title": "Dreambooth",
    "section": "Generate New Images",
    "text": "Generate New Images\n\npipe = StableDiffusionPipeline.from_pretrained(output_dir,safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\nprompt = \"a photo of sks dog in a bucket\"\n    \nimages = [pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0] for i in range(9)]\nclear_output()\nprint(prompt)\nshow_images(images)\n\na photo of sks dog in a bucket"
  },
  {
    "objectID": "Tutorials/callbacks.html",
    "href": "Tutorials/callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "The AIsaac library is an extremely flexible framework that uses callbacks a lot. They are probably more widely used than in any other framework. It’s super heavily influenced by callbacks system in the miniai library developed as part of the fastai course, but goes a bit further in that direction in a couple of aspects. Because of this it’s very important to understand how to use AIsaac uses them and how you can leverage that."
  },
  {
    "objectID": "Tutorials/callbacks.html#setup",
    "href": "Tutorials/callbacks.html#setup",
    "title": "Callbacks",
    "section": "Setup",
    "text": "Setup\nHere I will set up the needed pieces for the tutorial. This includes imports and loading a small subset of the fashion MNIST dataset.\n\nfrom AIsaac.all import *\nimport fastcore.all as fc\nimport matplotlib.pyplot as plt,matplotlib as mpl\nimport torch\nfrom datasets import load_dataset\nfrom torch import nn\nfrom torcheval.metrics import MulticlassAccuracy\nimport torchvision.transforms.functional as TF\n\n\ntorch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\ntorch.manual_seed(1)\nmpl.rcParams['image.cmap'] = 'gray'\nset_seed(42)\n\n\nxmean,xstd = 0.28, 0.35\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)\ndls = DataLoaders.from_dataset_dict(_dataset, 64, num_workers=4)\n\nFound cached dataset fashion_mnist (/home/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)"
  },
  {
    "objectID": "Tutorials/callbacks.html#basic-trainer",
    "href": "Tutorials/callbacks.html#basic-trainer",
    "title": "Callbacks",
    "section": "Basic Trainer",
    "text": "Basic Trainer\n\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.Adam, \n                  model = get_model_conv(),\n                  callbacks=[BasicTrainCB(),MetricsCB(Accuracy=MulticlassAccuracy()), DeviceCB(),ProgressCB()])\ntrainer.fit()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      0\n      0.601941\n      0.436749\n      0.8439\n    \n  \n\n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      1\n      0.389781\n      0.381396\n      0.858400\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      2\n      0.339881\n      0.348775\n      0.870900\n    \n  \n\n\n\nSo we passed in a DataLoaders, a pytorch loss, a pytorch optimizer, a pytorch model, and some callbacks. As you can see by running Trainer.fit it ran a full training loop. The training loop is defined entirely in the callbacks. For this tutorial we are focusing on the callbacks. Please refer to pytorch documentation for the pytorch pieces.\n\nOne batch\nLet’s see how a batch is processed. The source code for the batch trainer is very small and there’s two things we need to understand about it, the decorator and the run_callbacks method.\n@with_cbs('batch', CancelBatchException)\ndef one_batch(self):\n    self.run_callbacks(['predict','get_loss'])\n    if self.training: self.run_callbacks(['before_backward','backward','step','zero_grad'\n\nrun_callbacks\nThe run_callbacks method is what actually executes the callbacks code. As you can see a batch is just all callbacks.\nThe first run_callbacks does the following:\n\n\n\n\n\n\nrun_callbacks pseudo code\n\n\n\n\nSorts all callbacks according to the “order” attribute (defaults to 0)\nLoops through ['predict','get_loss']\n\nLoops through ordered callbacks:\n\nIf “predict” method exists for that callback then run it\n\n\n\n\n\nLet’s look at the BasicTrainCB code. Each element needed to process the batch is here. “before_backward” is not defined so this callback won’t do anything in that step. We could however define a callback that happens before the backward pass if we want to add functionality there to our training loop.\n\nview_source_code(BasicTrainCB)\n\n\n\n\n\n  \n  \n  \n\n\n\n\nclass BasicTrainCB:\n    '''Callback for basic pytorch training loop'''\n    def predict(self,trainer): trainer.preds = trainer.model(trainer.batch[0])\n    def get_loss(self,trainer): trainer.loss = trainer.loss_func(trainer.preds,trainer.batch[1])\n    def backward(self,trainer): trainer.loss.backward()\n    def step(self,trainer): trainer.opt.step()\n    def zero_grad(self,trainer): trainer.opt.zero_grad()\n\n\n\n\n\n\n\nwith_cbs\nWith cbs adds two pieces of functionality.\n\nAbility to exit and skip the rest of the function (ie the batch). This is similar to how you can use continue in a for loop. This can be done with raising the particular exception.\nAdds before, after, and cleanup callbacks to the function. Before and after run before and after the function. Cleanup will always run, even if an exception is thrown.\n\n\n\n\n\n\n\none_batch example\n\n\n\n\nTo run a callback before or after every batch, you would use before_batch and after_batch\nTo skip a batch, you would raise a CancelBatchException in a callback as that’s what is passed to the decorator.\nThe cleanup_batch callback will always run if one exists, even if you skipped the batch. after_batch will be skipped once the CancelBatchException is raised.\n\n\n\n\nview_source_code(with_cbs)\n\n\n\n\n\n  \n  \n  \n\n\n\n\nclass with_cbs:\n    def __init__(self, nm, exception): fc.store_attr()\n    def __call__(self, f):\n        def _f(o, *args, **kwargs):\n            try:\n                o.run_callbacks(f'before_{self.nm}')\n                f(o, *args, **kwargs)\n                o.run_callbacks(f'after_{self.nm}')\n            except self.exception: pass\n            finally: o.run_callbacks(f'cleanup_{self.nm}')\n        return _f\n\n\n\n\n\n\n\n\nOther Callbacks\nAnother example of a callback is the device callback, that puts things onto whatever device we want (ie GPU)\n\nview_source_code(DeviceCB)\n\n\n\n\n\n  \n  \n  \n\n\n\n\nclass DeviceCB:\n    '''Callback to train on specific device'''\n    def __init__(self, device=def_device): self.device=device\n    def before_fit(self, trainer):\n        '''Moves model to device'''\n        if hasattr(trainer.model, 'to'): trainer.model.to(self.device)\n    def before_batch(self, trainer): \n        '''moves batch to device'''\n        trainer.batch = to_device(trainer.batch, device=self.device)\n\n\n\n\n\n\nview_source_code(MetricsCB)\n\n\n\n\n\n  \n  \n  \n\n\n\n\nclass MetricsCB:\n    '''Callback to track train/valid loss + metrics'''\n    def __init__(self, **metrics):\n        self.metrics = metrics\n        self.losses = {'train':Mean(),'valid':Mean()}\n        self.metrics_epoch,self.losses_epoch,self.losses_batch = [],[],[]\n            \n    def after_batch(self,trainer):\n        '''stores losses and metrics for batch'''        \n        self.losses[f\"{'train' if trainer.training else 'valid'}\"].update(to_cpu(trainer.loss),weight=len(trainer.batch[1]))\n        if not trainer.training:\n            preds,batch = map(to_cpu,[trainer.preds,trainer.batch[1]])    \n            for k in self.metrics: self.metrics[k].update(preds,batch)\n        self.losses_batch.append({'training':trainer.training,'loss':to_cpu(trainer.loss)})\n            \n    def cleanup_epoch(self,trainer):\n        '''compute metrics and append to epoch stats and display'''\n        if not trainer.training:\n            self.metrics_epoch.append({name:float(metric.compute()) for name, metric in self.metrics.items()})\n            self.losses_epoch.append({name:float(metric.compute()) for name, metric in self.losses.items()})\n\n            for metric in self.metrics.values(): metric.reset()\n            for metric in self.losses.values(): metric.reset()\n\n\n\n\n\nIn addition, the MetricsCB in the example above is responsible for calculating and tracking the losses, the metrics it’s initalized with, and logging that every epoch.\nAll functionality that is done in the training loop is managed through callbacks.\nEpochs work similarly to batches with callbacks, and there is also a fit method which also executes callbacks in the same way.\n\n\n\n\n\n\nAvailable Callback List\n\n\n\n\nBatch callbacks\n\nbefore_batch\npredict\nget_loss\nbefore_backward\nbackward\nstep\nzero_grad\nafter_batch\ncleanup_batch\n\nEpoch callbacks\n\nbefore_epoch\nafter_epoch\ncleanup_epoch\n\nFit callbacks\n\nbefore_fit\nafter_fit\ncleanup_fit\n\n\n\n\n\n\n\n\n\n\nCancel Exceptions\n\n\n\n\nCancelBatchException\nCancelEpochException\nCancelFitException\n\n\n\n\n\nCallback Subclassing/Inheritance\nWe can inherit from the BasicTrainCB because momentum is mostly the same as a normal training loop with one small tweak that allows previous gradients to be accounted for. In this way we can build callbacks from other similar callbacks.\nRather than subclassing the Trainer, we subclass callbacks.\n\nview_source_code(MomentumTrainCB)\n\n\n\n\n\n  \n  \n  \n\n\n\n\nclass MomentumTrainCB(BasicTrainCB):\n    def __init__(self,momentum): self.momentum = momentum\n    def zero_grad(self,trainer): \n        '''Multiply grads by momentum (instead of zero)'''\n        with torch.no_grad():\n            for p in trainer.model.parameters(): p.grad *= self.momentum"
  },
  {
    "objectID": "Tutorials/callbacks.html#multiple-callbacks",
    "href": "Tutorials/callbacks.html#multiple-callbacks",
    "title": "Callbacks",
    "section": "Multiple Callbacks",
    "text": "Multiple Callbacks\nNow that we know how to modify and extend the training loop with individual callbacks, one next logical question is how to we create abstractions with this. For example, we probably don’t want to add DeviceCB, MetricsCB, and BasicTrainCB to every Trainer we create as lots of Trainers will use those. As we build more complex models we may want combinations of callbacks as well that are commonly used together, rather than having to memorize lots of callback recipes.\nTo do this, we create a recursive call when we add callbacks that allows us to group callbacks together. Instead of subclassing the Trainer in a way that may be more common in other frameworks, we group the callbacks together. We create these callbacks that are a combination of other callbacks, by defining the callbacks attribute in a callback.\n\n\n\n\n\n\nCallbacks Attribute\n\n\n\nAdding callbacks works recursively. Once a callback is added, it will check for a callbacks attribute and add those callbacks. Those in turn could have callbacks attributes of their own.\n\n\nHere is an example of a group of callbacks that will likely go together. This simple class will add all of these callbacks when used. While this class is not a callback itself because it does not have a callback method (ie before_batch), you have the flexibility to add those methods to this class to add behavior to your trainer specific to this grouping of callbacks. When passed as a Callback it will add the 5 callbacks stores in self.callbacks. It could also be a callback itsel\n\nview_source_code(CoreCBs)\n\n\n\n\n\n  \n  \n  \n\n\n\n\nclass CoreCBs:\n    def __init__(self,device=def_device,module_filter=fc.noop,**metrics):\n        self.callbacks = [DeviceCB(device=device),\n                          BasicTrainCB(),\n                          MetricsCB(**metrics),\n                          ProgressCB(),\n                          ActivationStatsCB(module_filter)]\n\n\n\n\n\n\ntrainer = Trainer(dls,\n                  nn.CrossEntropyLoss(), \n                  torch.optim.Adam, \n                  get_model_conv(),\n                  callbacks=[CoreCBs(Accuracy=MulticlassAccuracy(),)])\n\n\ntrainer.fit()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      0\n      0.599093\n      0.452553\n      0.8334\n    \n  \n\n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      1\n      0.381516\n      0.377897\n      0.862000\n    \n  \n\n\n\n\n\n\n  \n    \n       \n      train\n      valid\n      Accuracy\n    \n  \n  \n    \n      2\n      0.332677\n      0.343139\n      0.876300"
  },
  {
    "objectID": "Tutorials/vision.html",
    "href": "Tutorials/vision.html",
    "title": "Fashion MNIST",
    "section": "",
    "text": "from AIsaac.all import *\nimport torch\nfrom datasets import load_dataset\nfrom torch import nn\nfrom torcheval.metrics import MulticlassAccuracy\nimport torchvision.transforms.functional as TF"
  },
  {
    "objectID": "Tutorials/vision.html#basic-trainer",
    "href": "Tutorials/vision.html#basic-trainer",
    "title": "Fashion MNIST",
    "section": "Basic Trainer",
    "text": "Basic Trainer\n\nset_seed(1,True)\ntrainer = Trainer(dls,\n              nn.CrossEntropyLoss(), \n              torch.optim.Adam, \n              get_model_conv(norm=nn.BatchNorm2d),\n              callbacks=[CoreCBs(Accuracy=MulticlassAccuracy()),OneCycleSchedulerCB()])\ntrainer.fit(5,lr=.01)"
  },
  {
    "objectID": "Tutorials/vision.html#timm-model",
    "href": "Tutorials/vision.html#timm-model",
    "title": "Fashion MNIST",
    "section": "Timm Model",
    "text": "Timm Model\n\nmodel = get_model_timm('resnet18', pretrained=True,num_classes=10,in_chans=1)\n\n\nset_seed(1,True)\ntrainer = Trainer(dls,\n              nn.CrossEntropyLoss(), \n              torch.optim.Adam, \n              model,\n              callbacks=[CoreCBs(Accuracy=MulticlassAccuracy()),OneCycleSchedulerCB()])\ntrainer.fit(5,lr=.01)"
  },
  {
    "objectID": "Tutorials/vision.html#looking-at-trainer",
    "href": "Tutorials/vision.html#looking-at-trainer",
    "title": "Fashion MNIST",
    "section": "Looking at Trainer",
    "text": "Looking at Trainer\n\ntrainer.summarize_callbacks()"
  },
  {
    "objectID": "Tutorials/vision.html#looking-at-model",
    "href": "Tutorials/vision.html#looking-at-model",
    "title": "Fashion MNIST",
    "section": "Looking at Model",
    "text": "Looking at Model\n\ntrainer.summarize_model()"
  },
  {
    "objectID": "initialization.html",
    "href": "initialization.html",
    "title": "training",
    "section": "",
    "text": "xmean,xstd = 0.28, 0.35\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)"
  },
  {
    "objectID": "initialization.html#core",
    "href": "initialization.html#core",
    "title": "training",
    "section": "Core",
    "text": "Core\n\nsource\n\nlsuv_init\n\n lsuv_init (model, trainable_module, xb, measurement_module=None,\n            targ_mean=0.5, target_std=1, max_loops=100)\n\n\nmodel = get_model_conv()\nmodel.to(def_device)\nxb = fc.first(dls.train)[0]\nfor module in model.modules(): lsuv_init(model, module, xb.to(def_device))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AIsaac",
    "section": "",
    "text": "This library is a work in progress to become a deep learning library for my own use.\nThe code in this library is heavily inspired by the miniai library developed as part of the part 2 course. I am writing the Deep Learning portions myself, but other parts of the library are taken directly from that library (ie memory cleaning functions, plotting helper functions, etc.). This is very much a work in progress at the very early/beginning stages. It’s not ready for practical use (yet!)."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "AIsaac",
    "section": "Install",
    "text": "Install\nCone the repo and do an editable install. You will almost certainly need to modify the library as you use it given the early stages it is in.\npip install -e .[dev]"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "AIsaac",
    "section": "How to use",
    "text": "How to use\n\nDataloaders\n\nHuggingFace Dataset Dict\n\nfrom datasets import Dataset, load_dataset\nfrom AIsaac.dataloaders import DataLoaders\nimport torchvision.transforms.functional as TF\nfrom AIsaac.utils import inplace\n\nxmean,xstd = 0.28, 0.35\n\n@inplace\ndef transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n\n_dataset = load_dataset('fashion_mnist').with_transform(transformi)\ndls = DataLoaders.from_dataset_dict(_dataset, 64, num_workers=4)\ndls.show_batch(3)\n\nFound cached dataset fashion_mnist (/home/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n\n\n\n\n\n\n\n\n\n\nPytorch Datasets\n\n\n\nData Augmentation\n\nUse Existing Augmentations\n\nItem vs Batch Aug\nCPU vs GPU Aug\n\n\n\nAdd New Augmentation\n\nItem vs Batch Aug\nCPU vs GPU Aug\n\n\n\n\nTraining Loop\n\nCallback System\nModification Ex.\n\n\n\nModels\n\nPytorch Model\nTimm Model\nHuggingFace Model\n\n\n\nMetrics\n\nUse Existing Metric\nCreate New Metric\n\n\n\nOptimizers\n\nUse Existing Optimizer\nCreate New Optimizer\n\n\n\nLoss Functions\n\nUse Existing Loss Function\nCreate Loss Function\n\n\n\nModel Evaluations\n\nActivation Stats\n\nColor Dim\nDead Chart\nPlot Stats\n\nPlot Loss\nLogging with W&B"
  }
]