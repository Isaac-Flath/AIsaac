{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ce8972b6-838a-429d-a1b0-65bec08fc5a2",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de2b0e-c7a8-4c49-8bfb-c4669e9a47fb",
   "metadata": {},
   "source": [
    "# Dreambooth\n",
    "\n",
    "The goal of this tutorial is to get demo how to fine tune an image generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd75d55b-d644-4383-ab1e-872607c47394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/python3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "from AIsaac.all import *\n",
    "import fastcore.all as fc\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, PretrainedConfig,CLIPTextModel\n",
    "from accelerate import Accelerator\n",
    "import xformers # May need dev version\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from PIL import Image\n",
    "import itertools, math\n",
    "from pathlib import Path\n",
    "from itertools import zip_longest\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f3165e-ad75-4448-8731-d524a704d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_data_dir=Path(\"imgs/dreambooth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eac50c7-6923-4f23-9131-4fc85304801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokenizer, tokens, start_after='<|startoftext|>', end_at='<|endoftext|>'):\n",
    "    _decoded = ds.tokenizer.decode(tokens)\n",
    "    return _decoded[len(start_after):_decoded.find(end_at)].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2873fb43-23dc-45f8-8440-f83987c83e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm = UnNormalize([.5],[.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2d9875-abb5-4074-82e3-1e314abe93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b9d6d0c-d2f1-49b1-8fec-ebf3f71a66a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/home/.cache/huggingface/datasets/iflath___imagefolder/iflath--DogDreamBooth-fe88f623b33d9619/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "100%|██████████| 2/2 [00:00<00:00, 459.30it/s]\n"
     ]
    }
   ],
   "source": [
    "xmean,xstd = 0.5, 0.5\n",
    "@inplace\n",
    "def transformi(b): \n",
    "    b['image'] = [(TF.to_tensor(TF.resize(o,512))-xmean)/xstd for o in b['image']]\n",
    "    b['label'] = ['a photo of sks dog' for _ in b['label']]\n",
    "\n",
    "dd = load_dataset(\"iflath/DogDreamBooth\").with_transform(transformi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8fd3d3-3300-438d-90c6-54db97982acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#5) [Path('imgs/dreambooth/alvan-nee-bQaAJCbNq3g-unsplash.jpeg'),Path('imgs/dreambooth/alvan-nee-brFsZ7qszSY-unsplash.jpeg'),Path('imgs/dreambooth/alvan-nee-9M0tSjb-cpA-unsplash.jpeg'),Path('imgs/dreambooth/alvan-nee-Id1DBHv4fbg-unsplash.jpeg'),Path('imgs/dreambooth/alvan-nee-eoqnr8ikwFE-unsplash.jpeg')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_images(instance_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e223bb-7a94-4fb4-ae6b-b831daae7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model=\"CompVis/stable-diffusion-v1-4\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd80ab9f-06ae-487d-9c5a-5988856130ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = DreamBoothDataset(instance_data_dir,instance_prompt,tokenizer)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=1,shuffle=True,num_workers=0,)\n",
    "# xb = fc.first(train_dataloader)\n",
    "# [o.shape for o in xb.values()]\n",
    "# show_images(denorm(xb['instance_image']),titles=[decode(train_dataloader.dataset.tokenizer,o) for o in xb['instance_prompt_ids']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09348c69-07ef-4620-9d5e-60980ae25526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "919195a2-a04d-43fa-b04b-fbf88855e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(self,instance_data_dir,instance_prompt,tokenizer,img_size=512):\n",
    "        fc.store_attr('instance_data_dir,instance_prompt,tokenizer,img_size')\n",
    "        self.instance_images_path = get_images(instance_data_dir)\n",
    "\n",
    "    def __len__(self): return len(self.instance_images_path) \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = {}\n",
    "        instance_image = Image.open(self.instance_images_path[index])\n",
    "        \n",
    "        item_tfms = transforms.Compose([\n",
    "            transforms.CenterCrop(min(instance_image.size)), # Make square\n",
    "            transforms.Resize(self.img_size+64, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.RandomCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "        \n",
    "        batch['instance_image'] = item_tfms(instance_image)\n",
    "        \n",
    "        batch['instance_prompt_ids'] = self.tokenizer(self.instance_prompt,truncation=True,\n",
    "            padding=\"max_length\",max_length=self.tokenizer.model_max_length,return_tensors=\"pt\",\n",
    "            ).input_ids.squeeze()\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a53a626e-9dbc-47b9-aa58-d4649a234d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model,subfolder=\"tokenizer\",revision=None,use_fast=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34146dc8-30de-40eb-8b1b-4358d24fc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_prompt=\"a photo of ssskkksss dog\"\n",
    "# ds = DreamBoothDataset(instance_data_dir,instance_prompt,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c883a48e-480d-4f9d-8f91-02bb8af9a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DreamBoothDataset(instance_data_dir,instance_prompt,tokenizer)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=2,shuffle=True,num_workers=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50c6339e-835b-4025-8ea1-f807eaaf314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(pretrained_model, subfolder=\"text_encoder\", revision=None)\n",
    "text_encoder.requires_grad_(False)\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\", revision=None)\n",
    "vae.requires_grad_(False)\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model, subfolder=\"unet\", revision=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bcc7200-6d4f-42e4-9639-67ec59723593",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.enable_xformers_memory_efficient_attention()\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe05a4e5-4b6b-45aa-9c66-f7dbdb29e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(itertools.chain(unet.parameters(), text_encoder.parameters()),\n",
    "    lr=5e-6, betas=(0.9,0.999), weight_decay=1e-2, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfd73264-1588-4385-ad52-f8ad4009ebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = 400\n",
    "num_train_epochs = math.ceil(num_training_steps/len(train_dataloader))\n",
    "lr_scheduler = get_scheduler('constant',optimizer=optimizer,num_warmup_steps=0, num_training_steps=num_training_steps,num_cycles=1,power=1.,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc996619-1138-4484-bfee-417049596067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "unet,text_encoder,optimizer,train_dataloader,lr_scheduler=accelerator.prepare(unet,text_encoder,optimizer,train_dataloader,lr_scheduler)\n",
    "\n",
    "vae.to(accelerator.device, dtype=torch.float16)\n",
    "text_encoder.to(accelerator.device, dtype=torch.float16)\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d7c41c4-ba7e-440d-9272-aebae14d1c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = fc.bind(F.mse_loss,reduction=\"mean\")\n",
    "# optimizer = torch.optim.AdamW(itertools.chain(unet.parameters(), text_encoder.parameters()),\n",
    "#     lr=5e-6, betas=(0.9,0.999), weight_decay=1e-2, eps=1e-8)\n",
    "\n",
    "\n",
    "# Trainer(train_dataloader,loss_func,o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b2a137d-1d98-483e-afcf-c62eaffcab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothModel(nn.Module):\n",
    "    def __init__(self,unet,text_encoder): \n",
    "        super().__init__()\n",
    "        self.unet = nn.Sequential(unet)\n",
    "        self.text_encoder = nn.Sequential(text_encoder)\n",
    "        \n",
    "    def forward(self,x):        \n",
    "        encoder_hidden_states = text_encoder(x[\"instance_prompt_ids\"])[0]\n",
    "\n",
    "        # predict:  Predict the noise residual\n",
    "        return unet(x['noisy_latents'], x['timesteps'], encoder_hidden_states).sample.float()\n",
    "model = DreamBoothModel(unet,text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4535f886-47fb-4328-b489-04ddb151af03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 64, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['noisy_latents'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d74f83-688b-466e-93db-a0ce1dcb41e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 of 134 : Batch 0\n",
      "{'loss': 0.08537904918193817, 'lr': 5e-06}\n",
      "133 of 134 : Batch 1\n",
      "{'loss': 0.21098282933235168, 'lr': 5e-06}\n",
      "133 of 134 : Batch 2\n",
      "{'loss': 0.051845647394657135, 'lr': 5e-06}\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "# for epoch in range(2):\n",
    "    clear_output(wait=True); \n",
    "    unet.train()\n",
    "    text_encoder.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Convert images to latent space\n",
    "        batch['latents'] = vae.encode(batch[\"instance_image\"].float().to(dtype=torch.float16)).latent_dist.sample() * vae.config.scaling_factor\n",
    "\n",
    "        # Sample noise that we'll add to the latents\n",
    "        batch['noise'] = torch.randn_like(batch['latents']).float()\n",
    "        \n",
    "        # Sample a random timestep for each image\n",
    "        batch['timesteps'] = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch['latents'].shape[0],), device=batch['latents'].device).long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep\n",
    "        # (this is the forward diffusion process)\n",
    "        batch['noisy_latents'] = noise_scheduler.add_noise(batch['latents'], batch['noise'], batch['timesteps'])\n",
    "\n",
    "        \n",
    "        # # Get the text embedding for conditioning\n",
    "        encoder_hidden_states = text_encoder(batch[\"instance_prompt_ids\"])[0]\n",
    "        # # predict:  Predict the noise residual\n",
    "        \n",
    "        model_pred = unet(batch['noisy_latents'], batch['timesteps'], encoder_hidden_states).sample\n",
    "        # model_pred = model(batch)\n",
    "        \n",
    "        # loss: Get the loss \n",
    "        loss = F.mse_loss(model_pred, batch['noise'], reduction=\"mean\")\n",
    "\n",
    "        # backward\n",
    "        accelerator.backward(loss)\n",
    "            params_to_clip = (itertools.chain(unet.parameters(), text_encoder.parameters()))\n",
    "            accelerator.clip_grad_norm_(params_to_clip, 1.)\n",
    "\n",
    "        # Step\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Zero Grad\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logs = {\"loss\": to_cpu(loss).item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        print(f\"{epoch} of {num_train_epochs} : Batch {step}\")\n",
    "        print(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108df331-0341-4c2c-a0ea-068115c8ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"dreambooth/out\"\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    pretrained_model,\n",
    "    unet=accelerator.unwrap_model(unet),\n",
    "    text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "    revision=None,\n",
    ")\n",
    "pipeline.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f22fb6-a2ca-4456-8fbc-d539d9bb7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(output_dir, torch_dtype=torch.float16).to(\"cuda\")\n",
    "prompt = \"A photo of ssskkksss dog with a tennis ball\"\n",
    "    \n",
    "images = [pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0] for i in range(9)]\n",
    "clear_output()\n",
    "print(prompt)\n",
    "show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c1a0d-46e3-4205-9ca1-5b0a7a0757c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# class BasicTrainCB(Callback):\n",
    "#     '''Callback for basic pytorch training loop'''\n",
    "#     def predict(self,trainer): trainer.preds = trainer.model(trainer.batch[0])\n",
    "#     def get_loss(self,trainer): trainer.loss = trainer.loss_func(trainer.preds,trainer.batch[1])\n",
    "#     def backward(self,trainer): trainer.loss.backward()\n",
    "#     def step(self,trainer): trainer.opt.step()\n",
    "#     def zero_grad(self,trainer): trainer.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b176a1-5861-4095-bdee-6c081df70a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccelerateCB(BasicTrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, mixed_precision=\"fp16\"):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.acc = Accelerator(mixed_precision=mixed_precision)\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        '''Wraps model, opt, data in accelerate'''\n",
    "        learn.model,learn.opt,learn.dls.train,learn.dls.valid = self.acc.prepare(\n",
    "            learn.model, learn.opt, learn.dls.train, learn.dls.valid)\n",
    "\n",
    "        \n",
    "    def backward(self, learn): \n",
    "        '''Using accelerate for backward pass'''\n",
    "        self.acc.backward(learn.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49688a8b-efab-442c-9bc7-a73c044a756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothTrainCB:\n",
    "    \n",
    "    def predict(self,trainer): \n",
    "        encoder_hidden_states = text_encoder(trainer.batch[\"instance_prompt_ids\"])[0]\n",
    "        trainer.preds = unet(noisy_latents, timesteps, encoder_hidden_states).sample.float() # Predict the noise residual\n",
    "\n",
    "    def loss(self,trainer): \n",
    "        trainer.loss = F.mse_loss(trainer.preds, noise.float(), reduction=\"mean\")\n",
    "\n",
    "    def backward(self,trainer): \n",
    "        accelerator.backward(trainer.loss)\n",
    "        if accelerator.sync_gradients:\n",
    "            params_to_clip = (itertools.chain(unet.parameters(), text_encoder.parameters()))\n",
    "            accelerator.clip_grad_norm_(params_to_clip, 1.)\n",
    "            \n",
    "    def step(self,trainer):\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "    def zero_grad(self,trainer): pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    \n",
    "    def generate_latents(self):\n",
    "        latents = vae.encode(batch[\"instance_image\"].float().to(dtype=torch.float16)).latent_dist.sample()\n",
    "        latents = latents * vae.config.scaling_factor\n",
    "        \n",
    "    def noisify_latents(self):\n",
    "        # Sample noise that we'll add to the latents\n",
    "        noise = torch.randn_like(latents)\n",
    "        \n",
    "        # Sample a random timestep for each image\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=latents.device).long()\n",
    "\n",
    "        # Add noise to the latents according to the noise magnitude at each timestep (forward diffusion)\n",
    "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849ec176-8cbb-449c-a543-454bf6394dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
