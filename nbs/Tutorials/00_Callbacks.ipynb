{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ffbf61-9481-4824-8522-7200a88f7db1",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc79a30-d809-4631-b0bb-aee0f7c7c0cd",
   "metadata": {},
   "source": [
    "The `isaacai` library is an extremely flexible framework that uses callbacks **a lot**.  They are probably more widely used than in any other framework.  It's super heavily influenced by callbacks system in the `miniai` library developed as part of the fastai course, but goes a bit further in that direction in a couple of aspects.  Because of this it's very important to understand how to use `isaacai` uses them and how you can leverage that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8b0af-d88f-4258-9ae2-a12482056cbc",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here I will set up the needed pieces for the tutorial.  This includes imports and loading a small subset of the fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84cda67-fa0d-4a01-a9d0-c43a4e1263bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75d55b-d644-4383-ab1e-872607c47394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isaacai.all import *\n",
    "import fastcore.all as fc\n",
    "import matplotlib.pyplot as plt,matplotlib as mpl\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a860278-7a7d-477e-b755-ae7aad038b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b470376-1fb5-40da-aeff-94aa4dda9853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fashion_mnist (/home/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.032029151916503906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bab3a051d2455eb656e0717c18dfe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xmean,xstd = 0.28, 0.35\n",
    "@inplace\n",
    "def transformi(b): b['image'] = [(TF.to_tensor(o)-xmean)/xstd for o in b['image']]\n",
    "\n",
    "_dataset = load_dataset('fashion_mnist').with_transform(transformi)\n",
    "_dataset = sample_dataset_dict(_dataset)\n",
    "dls = DataLoaders.from_dataset_dict(_dataset, 64, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e400045d-2fb0-44b8-8794-110686c820df",
   "metadata": {},
   "source": [
    "## Basic Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730075e5-a3d0-46bb-8e2e-dbc96d47f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.5580000281333923, 'train_loss': 1.8707503662109375, 'valid_loss': 1.4094869384765625, 'epoch': 0, 'elapsed': datetime.timedelta(microseconds=238897)}\n",
      "{'Accuracy': 0.6539999842643738, 'train_loss': 1.0959883117675782, 'valid_loss': 1.026219970703125, 'epoch': 1, 'elapsed': datetime.timedelta(microseconds=222187)}\n",
      "{'Accuracy': 0.6819999814033508, 'train_loss': 0.793328239440918, 'valid_loss': 0.8982598266601562, 'epoch': 2, 'elapsed': datetime.timedelta(microseconds=178892)}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dls,\n",
    "                  nn.CrossEntropyLoss(), \n",
    "                  torch.optim.Adam, \n",
    "                  SimpleNet(28*28,64,10), \n",
    "                  callbacks=[BasicTrainCB(),MetricsCB(Accuracy=MulticlassAccuracy()), DeviceCB()])\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf2d5d-2531-4955-af61-d61b355bf4a0",
   "metadata": {},
   "source": [
    "So we passed in a `DataLoaders`, a pytorch loss, a pytorch optimizer, a pytorch model, and some callbacks.  As you can see by running `Trainer.fit` it ran a full training loop.  **The training loop is defined entirely in the callbacks**.  For this tutorial we are focusing on the callbacks.  Please refer to pytorch documentation for the pytorch pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45e8ee-ab8f-456c-8da5-28b74ceefa35",
   "metadata": {},
   "source": [
    "### One batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9a62c-8bcb-4938-9a91-2164c9bc5f48",
   "metadata": {},
   "source": [
    "Let's see how a batch is processed.  The source code for the batch trainer is very small and there's two things we need to understand about it, the decorator and the run_callbacks method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b832b365-6cbb-4c80-8b96-3bb260a533e0",
   "metadata": {},
   "source": [
    "```python\n",
    "@with_cbs('batch', CancelBatchException)\n",
    "def one_batch(self):\n",
    "    self.run_callbacks(['predict','get_loss'])\n",
    "    if self.training: self.run_callbacks(['before_backward','backward','step','zero_grad'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740fcda-2fd2-4f0b-854a-45ab1e9c7ef9",
   "metadata": {},
   "source": [
    "#### `run_callbacks`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296ffd3-ff3a-4a45-890c-3d0a294d7e46",
   "metadata": {},
   "source": [
    "The `run_callbacks` method is what actually executes the callbacks code.  As you can see a batch is just all callbacks. \n",
    "\n",
    "The first `run_callbacks` does the following:\n",
    "\n",
    "::: {.callout-tip}\n",
    "\n",
    "##### run_callbacks pseudo code\n",
    "+ Sorts all callbacks according to the \"order\" attribute (defaults to 0)\n",
    "+ Loops through `['predict','get_loss']`\n",
    "    + Loops through ordered callbacks:\n",
    "        + If \"predict\" method exists for that callback then run it\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a2a141-80ba-45f9-9fb4-96ed1f754609",
   "metadata": {},
   "source": [
    "Let's look at the `BasicTrainCB` code.  As you can see, each element needed to process the batch is here.  \"before_backward\" is not defined so this callback won't do anything in that step.  We could however define a callback that happens before the backward pass if we want to add functionality to our trainer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee048c94-e59d-4037-88fa-fcaa1a7bbb48",
   "metadata": {},
   "source": [
    "```python\n",
    "class BasicTrainCB:\n",
    "    def predict(self,trainer): trainer.preds = trainer.model(trainer.batch[0])\n",
    "    def get_loss(self,trainer): trainer.loss = trainer.loss_func(trainer.preds,trainer.batch[1])\n",
    "    def backward(self,trainer): trainer.loss.backward()\n",
    "    def step(self,trainer): trainer.opt.step()\n",
    "    def zero_grad(self,trainer): trainer.opt.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae90fc-f825-4738-a1a0-0b7d62bf0207",
   "metadata": {},
   "source": [
    "#### `with_cbs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3490c45-fd8d-44f0-bb99-7d6c11a8157c",
   "metadata": {},
   "source": [
    "With cbs adds two pieces of functionality.\n",
    "\n",
    "+ Ability to exit and skip the rest of the function (ie the batch).  This is similar to how you can use `continue` in a for loop.  This can be done with raising the particular `exception`.\n",
    "+ Adds before, after, and cleanup callbacks to the function.  Before and after run before and after the function.  Cleanup will always run, even if an exception is thrown.\n",
    "\n",
    "::: {.callout-tip}\n",
    "##### `one_batch` example\n",
    "\n",
    "+ To run a callback before or after every batch, you would use `before_batch` and `after_batch`\n",
    "+ To skip a batch, you would raise a `CancelBatchException` in a callback as that's what is passed to the decorator.\n",
    "+ The `cleanup_batch` callback will always run if one exists, even if you skipped the batch.  `after_batch` will be skipped once the `CancelBatchException` is raised.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a37e93-a55e-434e-87c1-fedaea39b862",
   "metadata": {},
   "source": [
    "```python\n",
    "class with_cbs:\n",
    "    def __init__(self, nm, exception): fc.store_attr()\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.run_callbacks(f'before_{self.nm}')\n",
    "                f(o, *args, **kwargs)\n",
    "                o.run_callbacks(f'after_{self.nm}')\n",
    "            except self.exception: pass\n",
    "            finally: o.run_callbacks(f'cleanup_{self.nm}')\n",
    "        return _f\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022dcfa-d131-40a8-9f23-968c441a2962",
   "metadata": {},
   "source": [
    "### Other Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca10213-7911-406b-acb5-cd91821ee516",
   "metadata": {},
   "source": [
    "Another example of a callback is the device callback, that puts things onto whatever device we want (ie GPU)\n",
    "\n",
    "```python\n",
    "class DeviceCB:\n",
    "    def __init__(self, device=def_device): self.def_device=def_device\n",
    "    def before_fit(self, trainer):\n",
    "        if hasattr(trainer.model, 'to'): trainer.model.to(self.device)\n",
    "    def before_batch(self, trainer): trainer.batch = to_device(trainer.batch, device=self.device)\n",
    "```    \n",
    "\n",
    "In addition, the `MetricsCB` in the example above is responsible for calculating and tracking the losses, the metrics it's initalized with, and logging that every epoch.\n",
    "\n",
    "**All functionality that is done in the training loop is managed through callbacks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4783f-5d9e-4d01-8d07-20431dc84286",
   "metadata": {},
   "source": [
    "Epochs work similarly to batches with callbacks, and there is also a fit method which also executes callbacks in the same way.  \n",
    "\n",
    "::: {.callout-tip}\n",
    "##### Available Callback List\n",
    "\n",
    "+ Batch callbacks\n",
    "    + before_batch\n",
    "    + predict\n",
    "    + get_loss\n",
    "    + before_backward\n",
    "    + backward\n",
    "    + step\n",
    "    + zero_grad\n",
    "    + after_batch\n",
    "    + cleanup_batch\n",
    "+ Epoch callbacks\n",
    "    + before_epoch\n",
    "    + after_epoch\n",
    "    + cleanup_epoch\n",
    "+ Fit callbacks\n",
    "    + before_fit\n",
    "    + after_fit\n",
    "    + cleanup_fit\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fc190-5396-4d6f-9982-4796f6226f3f",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "\n",
    "##### Cancel Exceptions\n",
    "\n",
    "+ CancelBatchException\n",
    "+ CancelEpochException\n",
    "+ CancelFitException\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84518a-d27b-40b0-83b6-bdacabd843bd",
   "metadata": {},
   "source": [
    "## Inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d2496-5732-4791-9347-d0d36e5f9892",
   "metadata": {},
   "source": [
    "Now that we know how to modify and extend the training loop, the next thing we want to do is subclass.  It would be really annoying if we had to remember too pass in the right combination of callbacks every time!  As an example, let's create a MomentumTrainer that has momentum using a GPU if it's available.  There's 2 steps for that.  \n",
    "\n",
    "1. Create a `MomentumTrainCB` similar to the `BasicTrainCB` above but implements momentum\n",
    "1. Create a `MomentumTrainer` similar to `Trainer` above that uses the appropriate callbacks without us having to add them in\n",
    "1. Update `__init__` function signature so we get information on any new parameters for those callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a075ef-be76-4964-bb5d-e4c1213fdc92",
   "metadata": {},
   "source": [
    "### MomentumTrainCB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efbe9d-c9e5-43cd-a3d4-3aff770a5b0a",
   "metadata": {},
   "source": [
    "We can inherit from the `BasicTrainCB` about because momentum is mostly the same as a normal training loop with one small tweak that allows previous gradients to be accounted for.\n",
    "\n",
    "```python\n",
    "class MomentumTrainCB(BasicTrainCB):\n",
    "    def __init__(self,momentum): self.momentum = momentum\n",
    "    def zero_grad(self,trainer): \n",
    "        with torch.no_grad():\n",
    "            for p in trainer.model.parameters(): p.grad *= self.mom\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9209e63-a455-4c06-aa29-40345ccdc7ee",
   "metadata": {},
   "source": [
    "### MomentumTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ab2b0-c6de-4c76-abf9-6d6a4d573a8b",
   "metadata": {},
   "source": [
    "Next we need to subclass `Trainer` and add the `MomentumTrainCB` and `DeviceCB` to the subclass.  For this we have a special class called `subclassing_method` that will run any arbitrary code on init, so we can add any callbacks (or anything else) there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af80aa-e78a-4126-a11f-53d63c37a6f0",
   "metadata": {},
   "source": [
    "```python\n",
    "@init_delegates()\n",
    "class MomentumTrainer(Trainer):\n",
    "    def subclassing_method(self,momentum=0.85,**kwargs):\n",
    "        super().subclassing_method(**kwargs)\n",
    "        self.add_callbacks([MomentumTrainCB(momentum),DeviceCB(),TrackingCB()])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc516f7d-f9fa-40e9-8c18-ba75731ed44c",
   "metadata": {},
   "source": [
    "The main problem with in this way is the `__init__` won't normally aware of the new argument `momentum`, so that argument will show up as `**kwargs` instead of showing the actual function.  While it wouldn't be difficult to look at the subclassing method for the arguments, what if you subclass MomentumTrainer?  You'd then need to go to the subclassing method of the parent class too. \n",
    "\n",
    "It's better if the init function signature said what all the arguments are all the way down the chain (subclassing method too) rather than having kwargs.  The `init_delegates` decorator handles that for us, as you can see below (momentum is added to the init signature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d5ca0-9923-46c1-9998-3dae1e25f416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Trainer.Init:   (self, dls, loss_func, opt_func, model, callbacks, **kwargs)\n",
      "MomentumTrainer.Init:   (self, dls, loss_func, opt_func, model, callbacks, momentum=0.85)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(f\"        Trainer.Init:   {inspect.signature(Trainer.__init__)}\")\n",
    "print(f\"MomentumTrainer.Init:   {inspect.signature(MomentumTrainer.__init__)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2413eaf-52d2-4f43-a63e-0394709d07f9",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "##### `init_delegates` limitations\n",
    "\n",
    "There are two known limitations of `init_delegates` currently.  I believe these aren't too hard to solve, but have not gotten to it yet.  If you want to see it fixed let me know and i'll jump on it.\n",
    "\n",
    "1. The subclassing_method arguments must have defaults\n",
    "1. The signature is not updating annotations\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa3cb4-6699-4540-9d13-8881bcb25c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
