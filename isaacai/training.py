# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/40_training.ipynb.

# %% auto 0
__all__ = ['run_callbacks', 'MetricsCB', 'DeviceCB', 'add_callbacks', 'BasicTrainCB', 'CancelFitException',
           'CancelBatchException', 'CancelEpochException', 'add_before_after_cbs', 'TrackingCB', 'Trainer',
           'MomentumTrainCB', 'InitDelegates', 'MomentumTrainer']

# %% ../nbs/40_training.ipynb 4
from .utils import *
from .dataloaders import *
from .models import *

from datetime import datetime, timedelta
import torchvision.transforms.functional as TF,torch.nn.functional as F
import math, time

import matplotlib.pyplot as plt
import matplotlib as mpl
import fastcore.all as fc
import torch
from torch import nn, Tensor
from datasets import load_dataset, Dataset
from torch.utils.data import DataLoader
import pandas as pd , numpy as np
from torcheval.metrics import MulticlassAccuracy,Mean

import dill as pickle
from torch.optim.lr_scheduler import ExponentialLR
from fastprogress.fastprogress import master_bar, progress_bar
import inspect


# %% ../nbs/40_training.ipynb 7
def run_callbacks(callbacks, method_name, trainer=None):
    for callback in sorted(callbacks, key=lambda x: getattr(x, 'order',0)):
        callback_method = getattr(callback, method_name,None)
        if callback_method is not None: callback_method(trainer)

# %% ../nbs/40_training.ipynb 8
class MetricsCB:
    def __init__(self, precision=4, **metrics):
        self._precision=precision
        self.metrics = metrics
        self.metrics.update({'train_loss':Mean(),'valid_loss':Mean()})
        
    def _log(self,x): print(x)
    
    def before_batch(self,trainer): self.batch_size = len(trainer.batch[1])
    
    def after_batch(self,trainer):
        if trainer.training: 
            self.metrics['train_loss'].update(to_cpu(trainer.loss),weight=self.batch_size)
        else: 
            self.metrics['valid_loss'].update(to_cpu(trainer.loss),weight=self.batch_size)
            for name, metric in self.metrics.items():
                if name in ('train_loss','valid_loss'): continue
                self.metrics[name].update(to_cpu(trainer.preds),to_cpu(trainer.batch[1]))
            
    def before_epoch(self,trainer):  self._st = datetime.now()
    def after_epoch(self,trainer):
        # compute metrics and append to epoch stats and display
        if not trainer.training:
            log = {name:float(metric.compute()) for name, metric in self.metrics.items()}
            log['epoch'] = trainer.epoch
            log['elapsed'] = datetime.now() - self._st
            self._log(log)
            [metric.reset() for metric in self.metrics.values()]

# %% ../nbs/40_training.ipynb 9
class DeviceCB:
    def __init__(self, device=def_device): fc.store_attr()
    def before_fit(self, trainer):
        if hasattr(trainer.model, 'to'): trainer.model.to(self.device)
    def before_batch(self, trainer): 
        trainer.batch = to_device(trainer.batch, device=self.device)

# %% ../nbs/40_training.ipynb 10
def add_callbacks(trainer,callbacks):
    trainer.callbacks = getattr(trainer,'callbacks',fc.L())
    if callbacks is not None:
        for callback in callbacks: 
            trainer.callbacks.append(callback.__class__.__name__)
            setattr(trainer,callback.__class__.__name__,callback)

# %% ../nbs/40_training.ipynb 11
class BasicTrainCB:
    def predict(self,trainer): trainer.preds = trainer.model(trainer.batch[0])
    def get_loss(self,trainer): trainer.loss = trainer.loss_func(trainer.preds,trainer.batch[1])
    def backward(self,trainer): trainer.loss.backward()
    def step(self,trainer): trainer.opt.step()
    def zero_grad(self,trainer): trainer.opt.zero_grad()

# %% ../nbs/40_training.ipynb 12
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/40_training.ipynb 13
def add_before_after_cbs(exception,callback_name):
    def decorator(func):
        def wrapper(*args,**kwargs):
            try: 
                self.run_callbacks(f"before_{callback_name}")
                return func(*args,**kwargs)
                self.run_callbacks(f"after_{callback_name}")
            except exception: pass
        return wrapper
    return decorator

# %% ../nbs/40_training.ipynb 15
class TrackingCB:
    def __init__(self):
        self._log_epoch,self._log_batch, self._log_fit = [pd.DataFrame()]*3
        
    def before_fit(self,trainer):
        trainer.epochs = master_bar(trainer.epochs)
        setattr(getattr(trainer,'MetricsCB'),'_log',self._log)
        log = {'model':str(trainer.model),'model_type':str(type(trainer.model)),'model_source':inspect.getsource(trainer.model.__class__)}
        log['callbacks'] = str(trainer.callbacks)

        for callback in trainer.callbacks:
            cb = getattr(trainer,callback)
            for cb_attr in dir(cb):
                if cb_attr.startswith('_'): continue
                if callable(getattr(cb,cb_attr)): continue
                log[cb_attr] = str(getattr(cb,cb_attr))
        self._log(log)
        
    def before_epoch(self,trainer):
        trainer.batches = progress_bar(trainer.batches,parent=trainer.epochs)
        trainer.epochs.child.comment = "Training" if trainer.training else "Validation"
        
    def _log(self,x):
        x = PPDict(x)        
        if 'epoch' in x.keys(): trainer.epochs.write(str(x))
        log = pd.DataFrame(x,index=[""])
        if 'batch' in x.keys(): self._log_batch = pd.concat([self._log_batch,log])
        elif 'epoch' in x.keys(): self._log_epoch = pd.concat([self._log_epoch,log])
        else: self._log_fit = pd.concat([self._log_fit,log])
        
        

# %% ../nbs/40_training.ipynb 16
class Trainer:
    def subclassing_method(self,**kwargs): pass

    @fc.delegates(to=subclassing_method)
    def __init__(self, dls, loss_func, opt_func, model, callbacks,**kwargs):
        self.add_callbacks(callbacks)
        fc.store_attr(but='callbacks')
        self.subclassing_method(**kwargs)
            
    @with_cbs('batch', CancelBatchException)
    def one_batch(self):
        self.run_callbacks(['predict','get_loss'])
        if self.training: self.run_callbacks(['before_backward','backward','step','zero_grad'])
    
    @with_cbs('epoch',CancelEpochException)
    def _one_epoch(self):
        for self.batch_num,self.batch in zip(self.batches,self.dl): self.one_batch()

    def one_epoch(self, training):
        self.model.train(training)
        self.dl = self.dls.train if training else self.dls.valid
        self.batches = range(len(self.dl))
        self._one_epoch()

    @with_cbs('fit', CancelFitException)
    def _fit(self,train, valid):
        for self.epoch in self.epochs: 
            if train: self.one_epoch(True)
            if valid: torch.no_grad()(self.one_epoch)(False)
    
    def fit(self, n_epochs=3, lr=1e-3, callbacks=None,train=True,valid=True):
        callbacks = fc.L()
        self.add_callbacks(callbacks)
        self.opt = self.opt_func(self.model.parameters(), lr)
        self.epochs = range(n_epochs)
        self._fit(train,valid)
        self.callbacks = [o for o in self.callbacks if o not in callbacks]
                                                        
    @property
    def training(self): return self.model.training
    
    def add_callbacks(self,callbacks): add_callbacks(self,callbacks)

    def run_callbacks(self,method_names): 
        cbs = [getattr(self,o) for o in self.callbacks]
        for method_name in fc.L(method_names): run_callbacks(cbs,method_name,self)

# %% ../nbs/40_training.ipynb 19
class MomentumTrainCB:
    def __init__(self,mom): fc.store_attr()
    def predict(self,trainer): trainer.preds = trainer.model(trainer.batch[0])
    def get_loss(self,trainer): trainer.loss = trainer.loss_func(trainer.preds,trainer.batch[1])
    def backward(self,trainer): trainer.loss.backward()
    def step(self,trainer): trainer.opt.step()
    def zero_grad(self,trainer): 
        with torch.no_grad():
            for p in trainer.model.parameters(): p.grad *= self.mom

# %% ../nbs/40_training.ipynb 20
def InitDelegates(learner,method='subclassing_method'):
    learner.__init__ = fc.delegates(getattr(learner,method))(learner.__init__)
    return learner   

# %% ../nbs/40_training.ipynb 21
@InitDelegates
class MomentumTrainer(Trainer): 
    def subclassing_method(self,precision=0.85): 
        self.add_callbacks([MomentumTrainCB(precision),DeviceCB(),TrackingCB()])

