# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/40_trainer.ipynb.

# %% auto 0
__all__ = ['CancelFitException', 'CancelBatchException', 'CancelEpochException', 'Trainer', 'init_delegates', 'BasicTrainCB',
           'DeviceCB', 'MetricsCB', 'TrackingCB', 'update_subclass_method_signature', 'update_init_method_signature',
           'MomentumTrainCB', 'MomentumTrainer', 'retrieve_global_name', 'LRFinderCB']

# %% ../nbs/40_trainer.ipynb 4
from .utils import *
from .dataloaders import *
from .models import *

from datetime import datetime, timedelta
import torchvision.transforms.functional as TF,torch.nn.functional as F
import math, time

import matplotlib.pyplot as plt
import matplotlib as mpl
import fastcore.all as fc
import torch
from torch import nn, Tensor
from datasets import load_dataset, Dataset
from torch.utils.data import DataLoader
import pandas as pd , numpy as np
from torcheval.metrics import MulticlassAccuracy,Mean

import dill as pickle
from torch.optim.lr_scheduler import ExponentialLR
from fastprogress.fastprogress import master_bar, progress_bar
import inspect


# %% ../nbs/40_trainer.ipynb 9
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/40_trainer.ipynb 10
class Trainer:
    def subclassing_method(self,**kwargs): pass

    def __init__(self, dls, loss_func, opt_func, model, callbacks,**kwargs):
        self.add_callbacks(callbacks)
        fc.store_attr(but='callbacks')
        self.subclassing_method(**kwargs)
            
    @with_cbs('batch', CancelBatchException)
    def one_batch(self):
        self.run_callbacks(['predict','get_loss'])
        if self.training: self.run_callbacks(['before_backward','backward','step','zero_grad'])
    
    @with_cbs('epoch',CancelEpochException)
    def _one_epoch(self):
        for self.batch_num,self.batch in zip(self.batches,self.dl): self.one_batch()

    def one_epoch(self, training):
        self.model.train(training)
        self.dl = self.dls.train if training else self.dls.valid
        self.batches = range(len(self.dl))
        self._one_epoch()

    @with_cbs('fit', CancelFitException)
    def _fit(self,train, valid):
        for self.epoch in self.epochs: 
            if train: self.one_epoch(True)
            if valid: torch.no_grad()(self.one_epoch)(False)
    
    def fit(self, n_epochs=3, lr=1e-3, callbacks=None,train=True,valid=True):
        callbacks = fc.L(callbacks)
        self.add_callbacks(callbacks)
        self.opt = self.opt_func(self.model.parameters(), lr)
        self.epochs = range(n_epochs)
        self._fit(train,valid)
        self.callbacks = [o for o in self.callbacks if o not in callbacks]
                                                        
    @property
    def training(self): return self.model.training
    
    def add_callbacks(self,callbacks,force=False): add_callbacks(self,callbacks,force)

    def run_callbacks(self,method_names): 
        cbs = [getattr(self,o) for o in self.callbacks]
        for method_name in fc.L(method_names): run_callbacks(cbs,method_name,self)

# %% ../nbs/40_trainer.ipynb 12
def init_delegates(trainer,method='subclassing_method'):
    '''Decorator to update a subclass init signature based on method'''
    trainer.__init__ = fc.delegates(getattr(trainer,method))(copy_func(trainer.__init__))
    return trainer   

# %% ../nbs/40_trainer.ipynb 13
class BasicTrainCB:
    '''Callback for basic pytorch training loop'''
    def predict(self,trainer): trainer.preds = trainer.model(trainer.batch[0])
    def get_loss(self,trainer): trainer.loss = trainer.loss_func(trainer.preds,trainer.batch[1])
    def backward(self,trainer): trainer.loss.backward()
    def step(self,trainer): trainer.opt.step()
    def zero_grad(self,trainer): trainer.opt.zero_grad()

# %% ../nbs/40_trainer.ipynb 14
class DeviceCB:
    '''Callback to train on specific device'''
    def __init__(self, device=def_device): self.device=device
    def before_fit(self, trainer):
        if hasattr(trainer.model, 'to'): trainer.model.to(self.device)
    def before_batch(self, trainer): trainer.batch = to_device(trainer.batch, device=self.device)

# %% ../nbs/40_trainer.ipynb 15
class MetricsCB:
    '''Callback to track train/valid loss + metrics'''
    def __init__(self, precision=4, **metrics):
        self._precision=precision
        self.metrics = metrics
        self.metrics.update({'train_loss':Mean(),'valid_loss':Mean()})
        
    def _log(self,x): print(x)
    
    def after_batch(self,trainer):
        self.batch_size = len(trainer.batch[1])
        if trainer.training: 
            self.metrics['train_loss'].update(to_cpu(trainer.loss),weight=self.batch_size)
        else: 
            self.metrics['valid_loss'].update(to_cpu(trainer.loss),weight=self.batch_size)
            for name, metric in self.metrics.items():
                if name in ('train_loss','valid_loss'): continue
                self.metrics[name].update(to_cpu(trainer.preds),to_cpu(trainer.batch[1]))
            
    def before_epoch(self,trainer):  self._st = datetime.now()
    def after_epoch(self,trainer):
        # compute metrics and append to epoch stats and display
        if not trainer.training:
            log = {name:float(metric.compute()) for name, metric in self.metrics.items()}
            log['epoch'] = trainer.epoch
            log['elapsed'] = datetime.now() - self._st
            self._log(log)
            [metric.reset() for metric in self.metrics.values()]

# %% ../nbs/40_trainer.ipynb 16
class TrackingCB:
    '''Callback to track and store hyperparameters, activation stats, and more'''
    def __init__(self):
        self._log_epoch,self._log_batch, self._log_fit = [pd.DataFrame()]*3
        
    def before_fit(self,trainer):
        trainer.epochs = master_bar(trainer.epochs)
        setattr(getattr(trainer,'MetricsCB'),'_log',self._log)
        log = {'model':str(trainer.model),'model_type':str(type(trainer.model)),'model_source':inspect.getsource(trainer.model.__class__)}
        log['callbacks'] = str(trainer.callbacks)

        for callback in trainer.callbacks:
            cb = getattr(trainer,callback)
            for cb_attr in dir(cb):
                if cb_attr.startswith('_'): continue
                if callable(getattr(cb,cb_attr)): continue
                log[cb_attr] = str(getattr(cb,cb_attr))
        self._log(log)
        
    def before_epoch(self,trainer):
        trainer.batches = progress_bar(trainer.batches,parent=trainer.epochs)
        trainer.epochs.child.comment = "Training" if trainer.training else "Validation"
        
    def _log(self,x):
        x = PPDict(x)        
        if 'epoch' in x.keys(): trainer.epochs.write(str(x))
        log = pd.DataFrame(x,index=[""])
        if 'batch' in x.keys(): self._log_batch = pd.concat([self._log_batch,log])
        elif 'epoch' in x.keys(): self._log_epoch = pd.concat([self._log_epoch,log])
        else: self._log_fit = pd.concat([self._log_fit,log])
        
        

# %% ../nbs/40_trainer.ipynb 21
def update_subclass_method_signature(subclass,method_name):
    subclass_params = dict(inspect.signature(getattr(subclass,method_name)).parameters)
    if 'kwargs' in subclass_params: subclass_params.pop('kwargs')
    parent_params = dict(inspect.signature(getattr(subclass.__base__,method_name)).parameters)
    parent_params = {k:v for k,v in parent_params.items() if k not in ['self','kwargs']}
    for k in parent_params: assert k not in subclass_params
    subclass_params.update(parent_params)
    getattr(subclass,method_name).__signature__ = inspect.signature(getattr(subclass,method_name)).replace(parameters=subclass_params.values())

# %% ../nbs/40_trainer.ipynb 22
def update_init_method_signature(subclass,method_name):
    setattr(subclass,'__init__',copy_func(subclass.__init__))
    init_params = dict(inspect.signature(getattr(subclass,'__init__')).parameters)
    if 'kwargs' in init_params: init_params.pop('kwargs')
    method_params = dict(inspect.signature(getattr(subclass,method_name)).parameters)
    method_params = {k:v for k,v in method_params.items() if (k not in ['self','kwargs']) and (k not in init_params)}          
    init_params.update(method_params)
    getattr(subclass,'__init__').__signature__ = inspect.signature(getattr(subclass,'__init__')).replace(parameters=init_params.values())

# %% ../nbs/40_trainer.ipynb 23
class init_delegates:
    def __init__(self, method='subclassing_method'): fc.store_attr()
    def __call__(self, c):
        update_subclass_method_signature(c, self.method)
        update_init_method_signature(c, self.method)        
        return c

# %% ../nbs/40_trainer.ipynb 25
class MomentumTrainCB(BasicTrainCB):
    def __init__(self,momentum): self.momentum = momentum
    def zero_grad(self,trainer): 
        with torch.no_grad():
            for p in trainer.model.parameters(): p.grad *= self.momentum

# %% ../nbs/40_trainer.ipynb 27
@init_delegates()
class MomentumTrainer(Trainer):
    def subclassing_method(self,momentum=0.85,**kwargs):
        super().subclassing_method(**kwargs)
        self.add_callbacks([MomentumTrainCB(momentum),DeviceCB(),TrackingCB()])

# %% ../nbs/40_trainer.ipynb 36
def retrieve_global_name(var):
    callers_local_vars = inspect.currentframe().f_back.f_back.f_locals.items()
    return [var_name for var_name, var_val in callers_local_vars if var_val is var]

# %% ../nbs/40_trainer.ipynb 37
class LRFinderCB:
    order = 1
    def __init__(self,lr_mult=1.3): fc.store_attr()
    
    def before_fit(self,trainer):
        pickle.dump(trainer,open('_tmp.pkl','wb'),protocol=pickle.HIGHEST_PROTOCOL)
        self.scheduler = ExponentialLR(optimizer=trainer.opt,gamma=self.lr_mult)
        self.lrs, self.losses = fc.L(), fc.L()
        self.min = math.inf
        
    def before_batch(self,trainer):
        if not trainer.training: raise CancelEpochException()
        
    def after_batch(self,trainer):
        self.lrs.append(trainer.opt.param_groups[0]['lr'])
        loss = to_cpu(trainer.loss)
        self.losses.append(loss)        
        if loss < self.min: self.min = loss
        if loss > self.min * 3: raise CancelEpochException()
        self.scheduler.step()
        
    def after_fit(self,_):
        plt.figure(figsize=(12,4))
        plt.plot(self.lrs,self.losses)
        plt.xscale('log')
        plt.title('Learning Rate Finder'); plt.xlabel('Learning Rate'); plt.ylabel('Losses') 
        plt.show()
        
    def cleanup_fit(self,trainer):
        _name = retrieve_global_name(trainer)
        globals()[_name[0]] = pickle.load(open('_tmp.pkl','rb'))
